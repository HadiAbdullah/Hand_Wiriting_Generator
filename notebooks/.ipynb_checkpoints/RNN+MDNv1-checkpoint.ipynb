{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "total_series_length = 5000\n",
    "truncated_backprop_length = 20\n",
    "state_size = 8\n",
    "num_classes = 2\n",
    "echo_step = 3\n",
    "batch_size = 5\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length\n",
    "num_layers = 6\n",
    "features = 1\n",
    "KMIX = 24\n",
    "NOUT = KMIX * 3\n",
    "NOUT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# _logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_mixture_coef(output):\n",
    "    out_pi = tf.placeholder(dtype=tf.float32, shape=[None,KMIX], name=\"mixparam\")\n",
    "    out_sigma = tf.placeholder(dtype=tf.float32, shape=[None,KMIX], name=\"mixparam\")\n",
    "    out_mu = tf.placeholder(dtype=tf.float32, shape=[None,KMIX], name=\"mixparam\")\n",
    "\n",
    "    out_pi, out_sigma, out_mu = tf.split(output, 3, 1)\n",
    "\n",
    "    max_pi = tf.reduce_max(out_pi, 1, keep_dims=True)\n",
    "    out_pi = tf.subtract(out_pi, max_pi)\n",
    "\n",
    "    out_pi = tf.exp(out_pi)\n",
    "\n",
    "    normalize_pi = tf.reciprocal(tf.reduce_sum(out_pi, 1, keep_dims=True))\n",
    "    out_pi = tf.multiply(normalize_pi, out_pi)\n",
    "\n",
    "    out_sigma = tf.exp(out_sigma)\n",
    "\n",
    "    return out_pi, out_sigma, out_mu\n",
    "oneDivSqrtTwoPI = 1 / math.sqrt(2*math.pi) # normalisation factor for gaussian, not needed.\n",
    "\n",
    "def tf_normal(y, mu, sigma):\n",
    "    y = tf.reshape(y,tf.shape(mu))\n",
    "\n",
    "    result = tf.subtract(y, mu)\n",
    "    result = tf.multiply(result,tf.reciprocal(sigma))\n",
    "    result = -tf.square(result)/2.0\n",
    "    return tf.multiply(tf.exp(result),tf.reciprocal(sigma))*oneDivSqrtTwoPI\n",
    "\n",
    "def get_lossfunc(out_pi, out_sigma, out_mu, y):\n",
    "    result1 = tf_normal(y, out_mu, out_sigma)\n",
    "    result2 = tf.multiply(result1, out_pi)\n",
    "    result3 = tf.reduce_sum(result2, 1, keep_dims=True)\n",
    "    result3 = result3+1e-10\n",
    "    result4 = -tf.log(result3)\n",
    "    return tf.reduce_mean(result4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-74f400d0c274>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m#         _loss= sess.run([lossfunc],feed_dict={batchX_placeholder: x_data.reshape([-1,20,1]), batchY_placeholder: y_data.reshape([-1,20])})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mbatchX_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruncated_backprop_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchY_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruncated_backprop_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlossfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mbatchX_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruncated_backprop_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchY_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruncated_backprop_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def generateData():\n",
    "    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n",
    "    y = np.roll(x, echo_step)\n",
    "    y[0:echo_step] = 0\n",
    "\n",
    "    x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
    "    y = y.reshape((batch_size, -1))\n",
    "\n",
    "    return (x, y)\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [None, truncated_backprop_length,features])\n",
    "batchY_placeholder = tf.placeholder(tf.float32, [None, truncated_backprop_length])\n",
    "init_state = tf.placeholder(tf.float32, [num_layers, 2, None, state_size])\n",
    "\n",
    "W2 = tf.Variable(np.random.rand(state_size, NOUT),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,NOUT)), dtype=tf.float32)\n",
    "\n",
    "initializer = tf.truncated_normal_initializer(-1,1)\n",
    "stacked_rnn = []\n",
    "for _ in range(num_layers):\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(state_size,features,initializer=initializer)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=0.5)\n",
    "    stacked_rnn.append(cell)\n",
    "\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell(stacked_rnn , state_is_tuple=True)\n",
    "states_series,current_state = tf.nn.dynamic_rnn(cell,batchX_placeholder,dtype=tf.float32)\n",
    "\n",
    "states_series = tf.reshape(states_series,[-1,state_size])\n",
    "logits = tf.matmul(states_series,W2)+b2\n",
    "\n",
    "\n",
    "out_pi, out_sigma, out_mu = get_mixture_coef(logits)\n",
    "lossfunc = get_lossfunc(out_pi, out_sigma, out_mu, batchY_placeholder)\n",
    "train_op = tf.train.AdamOptimizer().minimize(lossfunc)\n",
    "\n",
    "\n",
    "\"HERE\"\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "NEPOCH = 500\n",
    "loss = np.zeros(NEPOCH) # store the training progress here.\n",
    "for i in range(NEPOCH):\n",
    "#     _out_pi, _out_sigma, _out_mu,_logits,_lossfunc= sess.run([out_pi, out_sigma, out_mu,logits,lossfunc],feed_dict={batchX_placeholder: x_data.reshape([-1,20,1]), batchY_placeholder: y_data.reshape([-1,20])})\n",
    "#         _loss= sess.run([lossfunc],feed_dict={batchX_placeholder: x_data.reshape([-1,20,1]), batchY_placeholder: y_data.reshape([-1,20])})\n",
    "\n",
    "    sess.run(train_op,feed_dict={batchX_placeholder: x_data.reshape([-1,truncated_backprop_length,1]), batchY_placeholder: y_data.reshape([-1,truncated_backprop_length])})\n",
    "    loss[i] = sess.run(lossfunc, feed_dict={batchX_placeholder: x_data.reshape([-1,truncated_backprop_length,1]), batchY_placeholder: y_data.reshape([-1,truncated_backprop_length])})\n",
    "    \n",
    "#     print(\"PI \",a)\n",
    "#     print(\"stdDv \",b)\n",
    "#     print(\"Mean \",c)\n",
    "#     print(\"logits \",d)\n",
    "    print(\"loss \",loss[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test = np.float32(np.arange(-15,15,0.1))\n",
    "NTEST = x_test.size\n",
    "x_test = x_test.reshape(NTEST,1) # needs to be a matrix, not a vector\n",
    "\n",
    "def get_pi_idx(x, pdf):\n",
    "  N = pdf.size\n",
    "  accumulate = 0\n",
    "  for i in range(0, N):\n",
    "    accumulate += pdf[i]\n",
    "    if (accumulate >= x):\n",
    "      return i\n",
    "  print ('error with sampling ensemble')\n",
    "  return -1\n",
    "\n",
    "def generate_ensemble(out_pi, out_mu, out_sigma, M = 10):\n",
    "  NTEST = x_test.size\n",
    "  result = np.random.rand(NTEST, M) # initially random [0, 1]\n",
    "  rn = np.random.randn(NTEST, M) # normal random matrix (0.0, 1.0)\n",
    "  mu = 0\n",
    "  std = 0\n",
    "  idx = 0\n",
    "\n",
    "  # transforms result into random ensembles\n",
    "  for j in range(0, M):\n",
    "    for i in range(0, NTEST):\n",
    "      idx = get_pi_idx(result[i, j], out_pi[i])\n",
    "      mu = out_mu[i, idx]\n",
    "      std = out_sigma[i, idx]\n",
    "      result[i, j] = mu + rn[i, j]*std\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "out_pi_test, out_sigma_test, out_mu_test = sess.run(get_mixture_coef(logits), feed_dict={batchX_placeholder: x_test.reshape([-1, 20, 1])})\n",
    "\n",
    "y_test = generate_ensemble(out_pi_test, out_mu_test, out_sigma_test)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(x_data,y_data,'ro', x_test,y_test,'bo',alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.average(-np.log(_loss[0][1],dtype=np.float32),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(np.array(-np.log(_loss[0][1])).reshape([5000]),dtype=np.float128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = -np.log(_loss[0][2])\n",
    "np.average(x, weights=np.ones_like(x) / x.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x[x>10000] = 8008\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = _loss[0][3]\n",
    "np.where(x>10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_loss[0][0][4786]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
