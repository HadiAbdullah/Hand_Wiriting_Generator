{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementaiton of the Alex Graves Paper for Hand Writing Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sys \n",
    "import time\n",
    "import random\n",
    "\n",
    "import math as m\n",
    "sys.path.insert(0,'..')\n",
    "from utils import plot_stroke\n",
    "%matplotlib inline\n",
    "from collections import namedtuple\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MDN and RNN Params\n",
    "state_size = 300\n",
    "num_classes = 121\n",
    "num_layers = 3\n",
    "features = 3\n",
    "dropout_pkeep = 0.8\n",
    "gradient_clip = 10\n",
    "total_mixtures = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Training Params\n",
    "batch_size = 3\n",
    "back_prop_length = 100\n",
    "seqlength = 500\n",
    "NEPOCH = 2000\n",
    "batch_length = batch_size*seqlength*features\n",
    "save_path = \"../models/temp.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Structs\n",
    "normal_struct = namedtuple(\"normalStruct\", \"x mean sigma\")\n",
    "normal_struct_temp = namedtuple(\"normalStructTemp\", \"mean sigma\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load and Process Stroke Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_and_process_data(): \n",
    "    strokes = np.load('../data/strokes.npy')\n",
    "    texts = \"\"\n",
    "    with open('../data/sentences.txt') as f:\n",
    "        texts = f.readlines()\n",
    "\n",
    "    grand_strokes1 = []\n",
    "\n",
    "    for i in range(0,1000):\n",
    "        grand_strokes1 = np.append(grand_strokes1,strokes[i],)\n",
    "\n",
    "    grand_strokes2 = []\n",
    "    for i in range(1000,2000):\n",
    "        grand_strokes2 = np.append(grand_strokes2,strokes[i],)\n",
    "\n",
    "    grand_strokes3 = []\n",
    "    for i in range(2000,3000):\n",
    "        grand_strokes3 = np.append(grand_strokes3,strokes[i],)\n",
    "\n",
    "    grand_strokes4 = []\n",
    "    for i in range(3000,4000):\n",
    "        grand_strokes4 = np.append(grand_strokes4,strokes[i],)\n",
    "\n",
    "    grand_strokes5 = []\n",
    "    for i in range(4000,5000):\n",
    "        grand_strokes5 = np.append(grand_strokes5,strokes[i],)\n",
    "\n",
    "    grand_strokes6 = []\n",
    "    for i in range(5000,6000):\n",
    "        grand_strokes6 = np.append(grand_strokes6,strokes[i],)\n",
    "\n",
    "\n",
    "    g1 = np.append(grand_strokes1,grand_strokes2)\n",
    "    g2 = np.append(grand_strokes3,grand_strokes4)\n",
    "    g3 = np.append(grand_strokes5,grand_strokes6)\n",
    "\n",
    "    g12 = np.append(g1,g2)\n",
    "    grand_strokes = np.append(g12,g3)\n",
    "    grand_strokes = grand_strokes.reshape([-1,3])\n",
    "    print(\"Data Ready\")\n",
    "    return grand_strokes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Density Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDN Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Equation 18\n",
    "def get_stroke(eos_predicted):\n",
    "    eos_predicted =  tf.reciprocal(1+tf.exp(eos_predicted))  \n",
    "    return eos_predicted\n",
    "\n",
    "# Equation 19    \n",
    "def get_weights(weights):\n",
    "    weights = tf.nn.softmax(weights)\n",
    "    return weights\n",
    "\n",
    "# Equation 20    \n",
    "def get_mean(means):\n",
    "    return means\n",
    "\n",
    "# Equation 21    \n",
    "def get_stdv(standard_deviations):\n",
    "    out_sigma = tf.exp(standard_deviations)\n",
    "    return out_sigma\n",
    "\n",
    "# Equation 22    \n",
    "def get_correlation(correlations):\n",
    "    corr = tf.tanh(correlations)\n",
    "    return corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal1d(normal_struct):\n",
    "    x_mu = tf.subtract(normal_struct.x, normal_struct.mean)\n",
    "    return tf.square(tf.divide(x_mu, normal_struct.sigma))\n",
    "\n",
    "# Equation 25\n",
    "def Z(normal_struct1,normal_struct2, correlation):\n",
    "    x11 = normal1d(normal_struct1)\n",
    "    x12 = normal1d(normal_struct2)\n",
    "    x1x2 = tf.divide((tf.multiply(\n",
    "                      np.subtract(normal_struct1.x,normal_struct1.mean),\\\n",
    "                      tf.subtract(normal_struct2.x,normal_struct2.mean))),\\\n",
    "                     (tf.multiply(normal_struct1.sigma,normal_struct2.sigma)))\n",
    "\n",
    "    Z = x11+x12-(2*tf.multiply(correlation,x1x2))\n",
    "\n",
    "    return Z\n",
    "\n",
    "# Equation 24\n",
    "def normal2d(normal_struct1,normal_struct2, correlation):\n",
    "    pi_constant = tf.constant(m.pi)\n",
    "    \n",
    "    z = Z(normal_struct1,normal_struct2, correlation)\n",
    "    correlation_sqr = 1-(correlation*correlation)\n",
    "    expo = tf.exp(tf.div(-z,(2*(correlation_sqr))))\n",
    "\n",
    "    sigma = tf.multiply(normal_struct1.sigma,normal_struct2.sigma)\n",
    "    denominator = tf.reciprocal(2*pi_constant*tf.multiply(sigma, tf.sqrt(correlation_sqr)))\n",
    "    \n",
    "    result = tf.multiply(expo,denominator)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def loss_normal(gaussian, weights):\n",
    "    normal_loss1 = tf.multiply(gaussian, weights)\n",
    "    normal_loss2 = tf.reduce_sum(normal_loss1,1,keep_dims=True) \n",
    "    normal_loss2 = tf.maximum(normal_loss2, 1e-20) # This is done or loss will be NaN or Inf\n",
    "    normal_loss3 = -tf.log(normal_loss2) \n",
    "    \n",
    "    return normal_loss3\n",
    "\n",
    "def loss_eos(eos_actual,eos_predicted):\n",
    "    i = tf.constant(0)\n",
    "    values_eos =  tf.Variable([],dtype=tf.float32)\n",
    "    \n",
    "    # Applying correct function to EOS as per Equation 26\n",
    "    def condition(i,values_eos):\n",
    "        return i<(batch_size*back_prop_length)\n",
    "    \n",
    "    def body(i,values_eos):\n",
    "        result = tf.cond(eos_actual[i][0] < 1, lambda:-tf.log(1-eos_predicted[i]), lambda: -tf.log(eos_predicted[i]))\n",
    "        values_eos = tf.concat([values_eos,result],0)\n",
    "        return tf.add(i, 1),values_eos\n",
    "\n",
    "    # do the loop:\n",
    "    r,eos_result_log  = tf.while_loop(condition, body, [i,values_eos],shape_invariants=[i.get_shape(),\n",
    "                                                   tf.TensorShape([None])])\n",
    "    return eos_result_log\n",
    "\n",
    "# Equation 26\n",
    "def loss(weights,normal_struct1,normal_struct2, correlation, eos_actual, eos_predicted):\n",
    "\n",
    "    gaussian = normal2d(normal_struct1,normal_struct2, correlation)\n",
    "    normal = loss_normal(gaussian, weights)\n",
    "    \n",
    "    eos = loss_eos(eos_actual,eos_predicted)\n",
    "    eos = tf.reshape(eos,tf.shape(normal))\n",
    "    \n",
    "    result = tf.reduce_sum(normal + eos)\n",
    "    \n",
    "    return result,normal,eos,gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDN Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mdn_coeffs(mixture):\n",
    "\n",
    "        \n",
    "    eos_predicted = mixture[:, 0:1]\n",
    "    mixture = mixture[:, 1:]\n",
    "\n",
    "    #Split remaining values equally among the mix coeffs     \n",
    "    \"Section 4.2:(20 weights, 40 means, 40 standard deviations and 20 correlations)\"\n",
    "    weights, mean1, mean2, standard_deviation1, standard_deviation2, correlations = tf.split(mixture, 6, 1)              \n",
    "\n",
    "    # Apply functions to the extracted values\n",
    "    eos_predicted = get_stroke(eos_predicted)\n",
    "    weights = get_weights(weights)\n",
    "    mean1 = get_mean(mean1)\n",
    "    mean2 = get_mean(mean2)\n",
    "    sigma1 = get_stdv(standard_deviation1)\n",
    "    sigma2 = get_stdv(standard_deviation2)\n",
    "    correlation = get_correlation(correlations)\n",
    "    \n",
    "\n",
    "    normal_struct1 = normal_struct_temp( mean=mean1, sigma=sigma1)\n",
    "    normal_struct2 = normal_struct_temp( mean=mean2, sigma=sigma2)\n",
    "\n",
    "    return [eos_predicted, weights, correlation, normal_struct1, normal_struct2]\n",
    "\n",
    "    \n",
    "# Sample from the a bivariate sample of the given parameters\n",
    "def bivarate_normal_sample(random_seed,pi_index,_rho,_normal_struct1, _normal_struct2):\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    mean = [_normal_struct1.mean[0][pi_index], _normal_struct2.mean[0][pi_index]]\n",
    "\n",
    "    # Covariance Matrix: http://mathworld.wolfram.com/BivariateNormalDistribution.html\n",
    "    # |    (sigma1)^2       rho*sigma1*sigma2 |\n",
    "    # | rho*sigma1*sigma2       (sigma1)^2    | \n",
    "    covariance_matrix = [[np.square(_normal_struct1.sigma[0][pi_index]),\\\n",
    "                       (_rho[0][pi_index]*_normal_struct1.sigma[0][pi_index]*_normal_struct2.sigma[0][pi_index])],\\\n",
    "                      [(_rho[0][pi_index]*_normal_struct1.sigma[0][pi_index]*_normal_struct2.sigma[0][pi_index]),\\\n",
    "                       np.square(_normal_struct2.sigma[0][pi_index])]]\n",
    "\n",
    "    point = np.random.multivariate_normal(mean, covariance_matrix, 1).T\n",
    "\n",
    "    _x1 = point[0][0]\n",
    "    _x2 = point[1][0]\n",
    "    \n",
    "    return _x1,_x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X: The batch input coordinates [EOS,x1,x2]\n",
    "X = tf.placeholder(tf.float32, [None, None,features]) # [ batch_size, SEQLEN,features ]\n",
    "\n",
    "# Y: The predictions \n",
    "Y = tf.placeholder(tf.float32, [None,features]) # [ batch_size, SEQLEN,features ]\n",
    "\n",
    "# State of the RNN\n",
    "init_state = tf.placeholder(tf.float32, [num_layers, 2, None, state_size])\n",
    "\n",
    "# Weight and Bias tensors\n",
    "W = tf.Variable(np.random.rand(state_size, total_mixtures),dtype=tf.float32)\n",
    "bias = tf.Variable(np.zeros((1,total_mixtures)), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keeping track of intermediate state of the RNNs\n",
    "state_per_layer_list = tf.unstack(init_state, axis=0)\n",
    "rnn_tuple_state = tuple(\n",
    "            [tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])\n",
    "             for idx in range(num_layers)]\n",
    "        )\n",
    "\n",
    "# RNNs stacked ontop of each other\n",
    "'Section 4.2: three hidden layers, each consisting of 400 LSTM cells'\n",
    "stacked_rnn = []\n",
    "for _ in range(num_layers):\n",
    "    '4.2: network was retrained with adaptive weight noise ... with all std. devs. initialised to 0.075'\n",
    "    adaptive_weight_noise = tf.truncated_normal_initializer(stddev=0.075)\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(state_size)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout_pkeep)\n",
    "    stacked_rnn.append(cell)\n",
    "\n",
    "MultiRNNCell = tf.nn.rnn_cell.MultiRNNCell(stacked_rnn , state_is_tuple=True)\n",
    "states_series,current_state = tf.nn.dynamic_rnn(cell= MultiRNNCell,inputs= X, initial_state= rnn_tuple_state)\n",
    "# states_series: [ batch_size, SEQLEN, state_size ]\n",
    "# current_state  [ batch_size, state_size*num_layers ]\n",
    "\n",
    "states_series = tf.reshape(states_series, [-1, state_size])\n",
    "mixtures = tf.matmul(states_series, W) + bias \n",
    "# Split the Y \n",
    "[eos_actual,x1,x2] = tf.split(Y, features, 1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get Mixture coeffs\n",
    "coeffs = mdn_coeffs(mixtures)\n",
    "[eos_predicted, weights, correlation, normal_struct1, normal_struct2] = coeffs\n",
    "\n",
    "# Create a struct of mixture coeffs and coordinate values\n",
    "normal_struct1 = normal_struct(x=x1, mean=normal_struct1.mean, sigma=normal_struct1.sigma)\n",
    "normal_struct2 = normal_struct(x=x2, mean=normal_struct2.mean, sigma=normal_struct2.sigma)\n",
    "\n",
    "# Calcuate Loss\n",
    "calcualted_loss,_,_,_ = loss(weights,normal_struct1,normal_struct2, correlation, eos_actual, eos_predicted)\n",
    "\n",
    "# Normalize loss over batch\n",
    "# https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607\n",
    "calcualted_loss = tf.divide(calcualted_loss,batch_size)\n",
    "\n",
    "# Apply Gradient Clipping:\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.002)\n",
    "'Section 4.2: LSTM derivates were clipped in the range [−10, 10]'\n",
    "gradients, variables = zip(*optimizer.compute_gradients(calcualted_loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, gradient_clip)\n",
    "optimize = optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "# init = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data):\n",
    "    # Batches\n",
    "    start_batch = 0\n",
    "    end_batch = batch_length\n",
    "    batches = 10000000\n",
    "    _total_loss = []\n",
    "    epochs = 0\n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    grand_strokes = data\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "\n",
    "    _current_state = np.zeros((num_layers, 2, batch_size, state_size))\n",
    "\n",
    "    index = 0\n",
    "    while(True):\n",
    "        index += 1\n",
    "        print(index)\n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "        if(start_batch>grand_strokes.shape[0]):\n",
    "            epochs +=1\n",
    "            start_batch = start_batch%grand_strokes.shape[0]\n",
    "            end_batch = end_batch%grand_strokes.shape[0]\n",
    "            print(\"epochs \",epochs)\n",
    "\n",
    "\n",
    "        single_batch = grand_strokes.take(range(start_batch,end_batch),mode = \"wrap\")\n",
    "        single_batch = np.reshape(single_batch,[batch_size,seqlength,features])\n",
    "\n",
    "        start_batch += batch_size\n",
    "        end_batch += batch_size\n",
    "\n",
    "        # Mini Batches\n",
    "        mini_lenght = back_prop_length\n",
    "        start_mini = 0 \n",
    "        end_mini = mini_lenght\n",
    "\n",
    "        if(epochs>NEPOCH):\n",
    "            print(\"Complete\")\n",
    "            break\n",
    "\n",
    "        while(True):\n",
    "            if(end_mini+1>single_batch.shape[1]):\n",
    "                break\n",
    "\n",
    "            batchX = single_batch[:,start_mini:end_mini]\n",
    "            batchY = single_batch[:,start_mini+1:end_mini+1]\n",
    "            batchY = batchY.reshape([-1,3])\n",
    "            start_mini += 1\n",
    "            end_mini += 1\n",
    "            _,_loss,  __current_state =\\\n",
    "            sess.run([optimize,calcualted_loss, current_state],\n",
    "                 feed_dict={X: batchX, \n",
    "                            Y: batchY,\n",
    "                            init_state: _current_state })\n",
    "\n",
    "\n",
    "            _total_loss = np.append(_total_loss,_loss) \n",
    "\n",
    "            # Quit if a NaN is found in the Loss values\n",
    "            if(np.isnan(_loss)):\n",
    "                print(\"Naan found\")\n",
    "                sys.exit(0)\n",
    "            else:\n",
    "                _current_state = __current_state\n",
    "            \n",
    "\n",
    "        # Create a checkpoint in every iteration\n",
    "        saver.save(sess, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_unconditionally(random_seed=1):   \n",
    "    tf.set_random_seed(random_seed)\n",
    "    with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, save_path)\n",
    "\n",
    "            # Initiate coordinates and State\n",
    "            _x1 = 0\n",
    "            _x2 = 0\n",
    "            _eos = 1\n",
    "            prev_coordinates = np.array([[[_eos,_x1,_x2]]]) # shape should be [batch,seq_length,features]\n",
    "            _current_state = np.zeros((num_layers, 2, 1, state_size))\n",
    "\n",
    "\n",
    "            points = []\n",
    "            all_coordinates = []\n",
    "            for i in range(100):\n",
    "                \n",
    "                _eos_rounded= 1\n",
    "\n",
    "                # Run Session\n",
    "                feed = {X: prev_coordinates, init_state: _current_state}\n",
    "                [_eos, _pi, _rho, _normal_struct1, _normal_struct2],_current_state = sess.run([coeffs,current_state], feed_dict = feed)\n",
    "                \n",
    "                # Extract next point\n",
    "                pi_index = np.argmax(_pi)\n",
    "                _x1, _x2 = bivarate_normal_sample(random_seed, pi_index,_rho,_normal_struct1, _normal_struct2)\n",
    "                _eos_rounded = np.round(_eos)\n",
    "\n",
    "                if(np.isnan(_x1)|np.isnan(_x2)):\n",
    "                    continue\n",
    "\n",
    "                # Replace old point with new point\n",
    "                prev_coordinates = np.reshape([_eos_rounded,_x1,_x2],np.shape(prev_coordinates))\n",
    "\n",
    "                # Append new points to the list of all predicted points    \n",
    "                all_coordinates = np.append(all_coordinates,prev_coordinates)\n",
    "                all_coordinates = all_coordinates.reshape([-1,3])\n",
    "            return all_coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_and_process_data()\n",
    "train(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
